"""
RAG íŒŒì¼ëŸ¿ ì‹œìŠ¤í…œ - FastAPI ë°±ì—”ë“œ
"""
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import Optional, List
import os
import uuid
import tempfile

from rag_engine import rag_engine
from document_processor import process_document
from config import settings

app = FastAPI(
    title="RAG íŒŒì¼ëŸ¿ ì‹œìŠ¤í…œ",
    description="ë¡œì»¬ RAG ì‹œìŠ¤í…œ - NotebookLM ìŠ¤íƒ€ì¼",
    version="1.0.0"
)

# CORS ì„¤ì • (í”„ë¡ íŠ¸ì—”ë“œ ì—°ë™)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ========== ìš”ì²­/ì‘ë‹µ ëª¨ë¸ ==========

class ChatRequest(BaseModel):
    message: str
    
class ChatResponse(BaseModel):
    answer: str
    sources: List[dict]
    
class LLMSwitchRequest(BaseModel):
    provider: str  # "openai" or "ollama"


# ========== API ì—”ë“œí¬ì¸íŠ¸ ==========

@app.get("/")
async def root():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {
        "status": "running",
        "message": "ğŸš€ RAG íŒŒì¼ëŸ¿ ì‹œìŠ¤í…œì´ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤!",
        "llm_provider": rag_engine.llm_provider
    }


@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {"status": "healthy", "llm_provider": rag_engine.llm_provider}


@app.post("/upload")
async def upload_document(file: UploadFile = File(...)):
    """ë¬¸ì„œ ì—…ë¡œë“œ ë° ì²˜ë¦¬"""
    
    # ì§€ì› íŒŒì¼ í˜•ì‹ í™•ì¸
    allowed_extensions = ["txt", "pdf", "docx"]
    file_ext = file.filename.rsplit(".", 1)[-1].lower()
    
    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=400,
            detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ({', '.join(allowed_extensions)} ì§€ì›)"
        )
    
    try:
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_ext}") as temp_file:
            content = await file.read()
            temp_file.write(content)
            temp_path = temp_file.name
        
        # ë¬¸ì„œ ì²˜ë¦¬
        doc_data = process_document(temp_path, file.filename)
        
        # ê³ ìœ  ID ìƒì„±
        doc_id = str(uuid.uuid4())[:8]
        
        # ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€
        chunks_added = rag_engine.add_documents(
            doc_id=doc_id,
            chunks=doc_data["chunks"],
            metadata=doc_data["metadata"]
        )
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.unlink(temp_path)
        
        return {
            "success": True,
            "message": f"'{file.filename}' ì—…ë¡œë“œ ì™„ë£Œ!",
            "doc_id": doc_id,
            "chunks": chunks_added
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """RAG ê¸°ë°˜ ì±„íŒ…"""
    
    # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    search_results = rag_engine.search(request.message)
    
    if not search_results:
        return ChatResponse(
            answer="ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.",
            sources=[]
        )
    
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    contexts = [r["content"] for r in search_results]
    
    # LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±
    answer = rag_engine.generate_response(request.message, contexts)
    
    # ì†ŒìŠ¤ ì •ë³´
    sources = [
        {
            "source": r["metadata"].get("source", "Unknown"),
            "content_preview": r["content"][:100] + "..." if len(r["content"]) > 100 else r["content"]
        }
        for r in search_results
    ]
    
    return ChatResponse(answer=answer, sources=sources)


@app.get("/sources")
async def get_sources():
    """ì—…ë¡œë“œëœ ì†ŒìŠ¤ ëª©ë¡"""
    sources = rag_engine.get_all_sources()
    return {"sources": sources, "total": len(sources)}


@app.delete("/sources/{doc_id}")
async def delete_source(doc_id: str):
    """ì†ŒìŠ¤ ì‚­ì œ"""
    success = rag_engine.delete_source(doc_id)
    
    if success:
        return {"success": True, "message": f"ì†ŒìŠ¤ '{doc_id}' ì‚­ì œ ì™„ë£Œ"}
    else:
        raise HTTPException(status_code=404, detail="ì†ŒìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


@app.get("/sources/{doc_id}/content")
async def get_source_content(doc_id: str):
    """ì†ŒìŠ¤ ë¬¸ì„œì˜ ì „ì²´ ë‚´ìš© ì¡°íšŒ"""
    content = rag_engine.get_document_content(doc_id)
    
    if content:
        return content
    else:
        raise HTTPException(status_code=404, detail="ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


@app.post("/llm/switch")
async def switch_llm(request: LLMSwitchRequest):
    """LLM ì œê³µì ì „í™˜ (OpenAI / Ollama)"""
    result = rag_engine.switch_llm_provider(request.provider)
    return {"message": result, "current_provider": rag_engine.llm_provider}


@app.get("/llm/status")
async def llm_status():
    """í˜„ì¬ LLM ìƒíƒœ"""
    return {
        "provider": rag_engine.llm_provider,
        "openai_model": settings.OPENAI_MODEL,
        "ollama_model": settings.OLLAMA_MODEL
    }


@app.get("/ollama/models")
async def get_ollama_models():
    """Ollamaì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
    try:
        import ollama
        models = ollama.list()
        model_list = [
            {
                "name": model.get("name", model.get("model", "unknown")),
                "size": model.get("size", 0),
                "modified_at": model.get("modified_at", "")
            }
            for model in models.get("models", [])
        ]
        return {
            "models": model_list,
            "current_model": settings.OLLAMA_MODEL,
            "total": len(model_list)
        }
    except Exception as e:
        return {
            "models": [],
            "current_model": settings.OLLAMA_MODEL,
            "error": str(e),
            "total": 0
        }


class OllamaModelRequest(BaseModel):
    model: str


@app.post("/ollama/model")
async def set_ollama_model(request: OllamaModelRequest):
    """Ollama ëª¨ë¸ ë³€ê²½"""
    old_model = settings.OLLAMA_MODEL
    settings.OLLAMA_MODEL = request.model
    
    # RAG ì—”ì§„ì˜ ëª¨ë¸ë„ ì—…ë°ì´íŠ¸
    if rag_engine.llm_provider == "ollama":
        rag_engine._init_llm_client()
    
    return {
        "success": True,
        "message": f"Ollama ëª¨ë¸ì´ '{old_model}'ì—ì„œ '{request.model}'ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "current_model": settings.OLLAMA_MODEL
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
